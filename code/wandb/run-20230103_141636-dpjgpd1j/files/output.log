
wandb: ERROR Failed to serialize metric: Unknown Error
2023-01-03 14:17:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-01-03 14:17:57 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 4.000 GB ; name = NVIDIA GeForce RTX 3050 Laptop GPU
2023-01-03 14:17:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-01-03 14:19:39 | INFO | fairseq.tasks.translation | [en] dictionary: 8040 types
2023-01-03 14:19:39 | INFO | fairseq.tasks.translation | [zh] dictionary: 8040 types
2023-01-03 14:19:43 | INFO | simple_NMT_on_transformer | loading data for epoch 1
2023-01-03 14:19:43 | INFO | fairseq.data.data_utils | loaded 513,843 examples from: ./data/data-bin/translation2019zh_520000/train.en-zh.en
2023-01-03 14:19:43 | INFO | fairseq.data.data_utils | loaded 513,843 examples from: ./data/data-bin/translation2019zh_520000/train.en-zh.zh
2023-01-03 14:19:43 | INFO | fairseq.tasks.translation | ./data/data-bin/translation2019zh_520000 train en-zh 513843 examples
2023-01-03 14:19:43 | INFO | fairseq.data.data_utils | loaded 5,190 examples from: ./data/data-bin/translation2019zh_520000/valid.en-zh.en
2023-01-03 14:19:43 | INFO | fairseq.data.data_utils | loaded 5,190 examples from: ./data/data-bin/translation2019zh_520000/valid.en-zh.zh
2023-01-03 14:19:43 | INFO | fairseq.tasks.translation | ./data/data-bin/translation2019zh_520000 valid en-zh 5190 examples
{'id': 1,
 'source': tensor([ 173,  784,   86,    7,   35,  492,  262,   11,  477,  781,   34,  213,
          18,  176,   63,   31,   53,   60,    4,  236,    7,  791,   26,  797,
          66,   10,  413,  157,   33,  138,  233,  153,   36,   89,   17,   13,
        1138,  367,   15,  237,   55,   69,  983, 1455,  199,   42,   21,   14,
           7,   88,   52,   66,   10,  964,   44,   53, 1026,  116,   35,  201,
          57,  171,  153,  662,  155,    8,    2]),
 'target': tensor([   5,  531,  261, 2192, 2093, 1963,   91, 1051, 2711,    4,  547,  267,
        1562, 2370, 1059, 2032,    6,  583, 1289, 1463, 1615,  223,   43, 1872,
         178, 1453, 1995,  528,  229,  116,  801, 1721, 1453,  420, 1166,  212,
         261,  329, 2020,   56,  768,  519, 1241,    9,    2])}
('Source: at present the direction of travel is not fully clear , but theresa '
 "may's government has promised to set out a plan before triggering the eu's "
 'article 50 divorce procedure .')
'Target: 目前轨迹尚不清晰 , 但特雷莎·梅的政府承诺将在启动欧盟第50条脱欧程序之前制订一项计划 。'
2023-01-03 14:20:44 | WARNING | fairseq.tasks.fairseq_task | 4,805 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[3504, 4693, 5102, 3353, 4268, 459, 4933, 3053, 1509, 3267]
2023-01-03 14:46:10 | INFO | simple_NMT_on_transformer | Seq2Seq(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8040, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8040, 256, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=256, out_features=8040, bias=False)
  )
)
Tue Jan  3 14:53:29 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.57       Driver Version: 497.53       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |
| N/A   39C    P8     3W /  N/A |    580MiB /  4096MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | task: TranslationTask
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | encoder: TransformerEncoder
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | decoder: TransformerDecoder
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | criterion: LabelSmoothedCrossEntropyCriterion
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | optimizer: NoamOpt
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | num. model params: 11,490,304 (num. trained: 11,490,304)
2023-01-03 14:53:33 | INFO | simple_NMT_on_transformer | max tokens per batch = 8192, accumulate steps = 2
